{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Submitted By: `Shreeyash Pardeshi`**"
      ],
      "metadata": {
        "id": "O5wzvLiSkEyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "Kt_0xUc3bvf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 1\n",
        " Take any YouTube videos link and your task is to extract the comments from\n",
        "that videos and store it in a csv file and then you need define what is most\n",
        "demanding topic in that videos comment section."
      ],
      "metadata": {
        "id": "YvBJUuFyb24c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsYuEgbpg3Ec",
        "outputId": "01955717-3a17-4908-c9dc-3e74eca58b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python -m nltk.downloader stopwords"
      ],
      "metadata": {
        "id": "Z4fjgq3Hgpto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "# Get the YouTube video URL\n",
        "video_url = \"https://www.youtube.com/watch?v=kEmnkUw0NTs\"\n",
        "\n",
        "# Get the video ID\n",
        "video_id = video_url.split(\"/\")[-1]\n",
        "\n",
        "# Create a request to get the comments for the video\n",
        "request = requests.get(\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=\" + video_id)\n",
        "\n",
        "# Get the response\n",
        "response = request.json()\n",
        "\n",
        "# Check if the response contains any items\n",
        "if \"items\" not in response:\n",
        "    print(\"Error: no comments found\")\n",
        "else:\n",
        "    # Create a list to store the comments\n",
        "    comments = []\n",
        "\n",
        "    # Iterate through the comments\n",
        "    for comment in response[\"items\"]:\n",
        "        # Add the comment to the list\n",
        "        try:\n",
        "            comments.append(comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"])\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    # Create a CSV file to store the comments\n",
        "    with open(\"comments.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "\n",
        "        # Create a CSV writer object\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write the comments to the CSV file\n",
        "        writer.writerow(comments)\n",
        "\n",
        "    # Determine the most demandingtopic in the comment section\n",
        "    # Create a dictionary to store the count of each topic\n",
        "    topic_count = {}\n",
        "\n",
        "    # Iterate through the comments\n",
        "    for comment in comments:\n",
        "\n",
        "        # Get the topic of the comment\n",
        "        topic = comment.split(\" \")[0]\n",
        "\n",
        "        # Increment the count for the topic in the dictionary\n",
        "        if topic in topic_count:\n",
        "            topic_count[topic] += 1\n",
        "        else:\n",
        "            topic_count[topic] = 1\n",
        "\n",
        "    # Determine the most demanding topic\n",
        "    most_demanding_topic = max(topic_count, key=topic_count.get)\n",
        "\n",
        "    # Print the most demanding topic\n",
        "    print(\"The most demanding topic in the comment section is:\", most_demanding_topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuppoiDtb9G-",
        "outputId": "3a22a47f-df84-44f9-98e0-85f9414b3b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: no comments found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 2:\n",
        "Take any pdf and your task is to extract the text from that pdf and store it in a\n",
        "csv file and then you need to find the most repeated word in that pdf.\n",
        "\n"
      ],
      "metadata": {
        "id": "naE8dmOubzfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf"
      ],
      "metadata": {
        "id": "gPABPSq2bW25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAz5eptcaC6t",
        "outputId": "4607e99d-2ea2-49e5-ad5f-8b1ec25bb38e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Repeated Word: the\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "import fitz\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_file = '/content/mystery_short_stories.pdf'\n",
        "doc = fitz.open(pdf_file)\n",
        "\n",
        "# Extract text from each page of the PDF\n",
        "text = ''\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "\n",
        "# Remove newlines and extra whitespaces\n",
        "text = text.replace('\\n', ' ').strip()\n",
        "\n",
        "# Store the extracted text in a CSV file\n",
        "csv_file = 'extracted_text.csv'\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Text'])\n",
        "    writer.writerow([text])\n",
        "\n",
        "# Find the most repeated word\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "most_common_word = word_counts.most_common(1)[0][0]\n",
        "\n",
        "# Print the most repeated word\n",
        "print('Most Repeated Word:', most_common_word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "import fitz\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_file = '/content/mystery_short_stories.pdf'\n",
        "doc = fitz.open(pdf_file)\n",
        "\n",
        "# Extract text from each page of the PDF\n",
        "text = ''\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "\n",
        "# Remove newlines and extra whitespaces\n",
        "text = text.replace('\\n', ' ').strip()\n",
        "\n",
        "# Store the extracted text in a CSV file\n",
        "csv_file = 'extracted_text.csv'\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Text'])\n",
        "    writer.writerow([text])\n",
        "\n",
        "# Find the most repeated word\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "most_common_word = word_counts.most_common(1)[0][0]\n",
        "\n",
        "# Print the most repeated word\n",
        "print('Most Repeated Word:', most_common_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jsHKI1Ica-J",
        "outputId": "44c08b61-e165-4fe3-cf75-8f8244644326"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Repeated Word: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('/content/extracted_text.csv')"
      ],
      "metadata": {
        "id": "FEauPF2YUUv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 3:\n",
        " from question 2, As you got the CSV and now you need perform key word\n",
        "extraction from that csv file and do the Topic modeling\n"
      ],
      "metadata": {
        "id": "77SHFqx1cPOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTW-qR1WU_v",
        "outputId": "0ecc3338-8e2b-4f96-8e8f-07b74406c40c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Download the punkt resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Increase the maximum field size limit\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# Load the extracted text from the CSV file\n",
        "csv_file = 'extracted_text.csv'\n",
        "with open(csv_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)  # Skip the header row\n",
        "    text = next(reader)[0]\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Create a dictionary of words and their frequencies\n",
        "word_counts = nltk.FreqDist(words)\n",
        "\n",
        "# Print the 10 most common words and their frequencies\n",
        "print('Most common words:')\n",
        "for word, count in word_counts.most_common(10):\n",
        "    print(f'{word}: {count}')\n",
        "\n",
        "# Create a bag-of-words representation of the text\n",
        "dictionary = corpora.Dictionary([words])\n",
        "corpus = [dictionary.doc2bow(words)]\n",
        "\n",
        "# Perform topic modeling using LDA\n",
        "num_topics = 5\n",
        "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "# Print the top 5 topics and their keywords\n",
        "print(f'Top {num_topics} topics:')\n",
        "for idx, topic in lda_model.print_topics(num_topics=num_topics):\n",
        "    print(f'Topic {idx}: {topic}')"
      ],
      "metadata": {
        "id": "zJc6-T_VcnaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2543e3ff-bd3e-4169-9b70-d998204306c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words:\n",
            ",: 3863\n",
            ".: 3182\n",
            "``: 1286\n",
            "'': 1272\n",
            "?: 433\n",
            ";: 379\n",
            "'s: 341\n",
            "said: 269\n",
            "!: 251\n",
            "--: 241\n",
            "Top 5 topics:\n",
            "Topic 0: 0.064*\",\" + 0.058*\".\" + 0.023*\"``\" + 0.017*\"''\" + 0.010*\"?\" + 0.009*\";\" + 0.006*\"'s\" + 0.005*\"!\" + 0.005*\"n't\" + 0.004*\"man\"\n",
            "Topic 1: 0.074*\".\" + 0.069*\",\" + 0.037*\"''\" + 0.025*\"``\" + 0.009*\"?\" + 0.008*\"said\" + 0.007*\"'s\" + 0.007*\";\" + 0.007*\"!\" + 0.006*\"--\"\n",
            "Topic 2: 0.065*\",\" + 0.048*\".\" + 0.022*\"''\" + 0.021*\"``\" + 0.008*\"?\" + 0.005*\"!\" + 0.005*\"'s\" + 0.005*\";\" + 0.004*\"one\" + 0.004*\"--\"\n",
            "Topic 3: 0.098*\",\" + 0.076*\".\" + 0.035*\"``\" + 0.021*\"''\" + 0.009*\"?\" + 0.008*\"'s\" + 0.008*\";\" + 0.006*\"n't\" + 0.006*\"said\" + 0.005*\"--\"\n",
            "Topic 4: 0.090*\",\" + 0.065*\".\" + 0.032*\"''\" + 0.025*\"``\" + 0.010*\";\" + 0.010*\"?\" + 0.007*\"'s\" + 0.006*\"said\" + 0.006*\"n't\" + 0.005*\"--\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 4:\n",
        "Take any text file and now your task is to Text Summarization without using\n",
        "hugging transformer library\n"
      ],
      "metadata": {
        "id": "8XogovIgkpN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "\n",
        "# Function to calculate sentence importance based on word frequency\n",
        "def calculate_sentence_scores(sentences, word_frequencies):\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if len(sentence.split()) < 30:  # Consider only sentences with less than 30 words\n",
        "                    if sentence not in sentence_scores:\n",
        "                        sentence_scores[sentence] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += word_frequencies[word]\n",
        "    return sentence_scores\n",
        "\n",
        "# Function to perform text summarization\n",
        "def summarize_text(text, num_sentences):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    \n",
        "    # Tokenize the text into words\n",
        "    words = [word.lower() for word in word_tokenize(text) if word not in stopwords.words('english') and word not in punctuation]\n",
        "    \n",
        "    # Calculate word frequencies\n",
        "    word_frequencies = nltk.FreqDist(words)\n",
        "    \n",
        "    # Calculate sentence scores based on word frequencies\n",
        "    sentence_scores = calculate_sentence_scores(sentences, word_frequencies)\n",
        "    \n",
        "    # Select the top N sentences with the highest scores as the summary\n",
        "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    \n",
        "    # Join the summary sentences into a single string\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/Transcript_extraction', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Set the number of sentences for the summary\n",
        "num_sentences = 3\n",
        "\n",
        "# Generate the summary\n",
        "summary = summarize_text(text, num_sentences)\n",
        "\n",
        "# Print the summary\n",
        "print(\"Text Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edky2fa_ktUN",
        "outputId": "e1e82e40-cb3d-4a55-ebed-4209ccbac0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Summary:\n",
            "Increased rigor and focus on external certifications resulted in 53,000 TCSes  acquiring certifications on hyperscaler cloud skills during the year, bringing the\"\n",
            "Rajesh Gopinathan:,\"Thank you, NGS. Thank you.\" Or should we think about this as  something similar to furloughs and we will see this come back in a quarter or  two?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 5\n",
        " Now you need build your own language detection with the fast Text model by Facebook \n"
      ],
      "metadata": {
        "id": "QdpwuWzgsSLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "id": "Yb5vG6g2WqJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Prepare the sample sentences\n",
        "sentences = [\n",
        "    'This is an example sentence in English',\n",
        "    'Ceci est une phrase en français',\n",
        "    'Dies ist ein Satz auf Deutsch'\n",
        "]\n",
        "\n",
        "# Prepare the labeled data in FastText format\n",
        "labeled_data = []\n",
        "for sentence in sentences:\n",
        "    labeled_data.append(f'__label__en {sentence}')  # English sentences labeled as '__label__en'\n",
        "    labeled_data.append(f'__label__fr {sentence}')  # French sentences labeled as '__label__fr'\n",
        "    labeled_data.append(f'__label__de {sentence}')  # German sentences labeled as '__label__de'\n",
        "\n",
        "# Write the labeled data to a file\n",
        "with open('labeled_data.txt', 'w') as f:\n",
        "    for line in labeled_data:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "# Train the FastText model\n",
        "model = fasttext.train_supervised(input='labeled_data.txt', lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='softmax')\n",
        "\n",
        "# Test the model on some example sentences\n",
        "sentences_to_predict = [\n",
        "    'This is an English sentence.',\n",
        "    'Dies ist ein Satz auf Deutsch.',\n",
        "]\n",
        "\n",
        "for sentence in sentences_to_predict:\n",
        "    prediction = model.predict(sentence)\n",
        "    predicted_label = prediction[0][0]\n",
        "    predicted_language = predicted_label.split('__label__')[1]\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted Language: {predicted_language}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ByjcZhZI1NDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61c062d-d06c-42f6-83d3-b40fb4859e15"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This is an English sentence.\n",
            "Predicted Language: en\n",
            "\n",
            "Sentence: Dies ist ein Satz auf Deutsch.\n",
            "Predicted Language: fr\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 6:\n",
        "Generate research papers titles using Bert model and containerize the\n",
        "application and push to public docker hub\n"
      ],
      "metadata": {
        "id": "qVGeal_-ZBT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "import os\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Generate research paper titles\n",
        "def generate_title(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids, max_length=20, num_return_sequences=1)\n",
        "    generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_title\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Recent advances in deep learning\"\n",
        "generated_title = generate_title(input_text)\n",
        "print(generated_title)"
      ],
      "metadata": {
        "id": "hLd0kLz79r_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ea3e57-2a6e-4a29-f311-6b1d833ef298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recent advances in deep learning.............\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dockerfile content\n",
        "dockerfile_content = '''\n",
        "FROM python:3.9\n",
        "WORKDIR /app\n",
        "COPY . /app\n",
        "RUN pip install torch transformers\n",
        "CMD [ \"python\", \"app.py\" ]\n",
        "'''"
      ],
      "metadata": {
        "id": "Jo-9mqEdaRvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the code to a file\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write('''import torch\n",
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_title(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids, max_length=20, num_return_sequences=1)\n",
        "    generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_title\n",
        "\n",
        "input_text = \"Recent advances in deep learning\"\n",
        "generated_title = generate_title(input_text)\n",
        "print(generated_title)\n",
        "''')"
      ],
      "metadata": {
        "id": "AN3hvHbiakF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the Dockerfile\n",
        "with open('Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "# Build the Docker image\n",
        "os.system('docker build -t research-titles .')\n",
        "\n",
        "# Log in to Docker Hub\n",
        "docker_username = 'your-dockerhub-username'\n",
        "os.system(f'docker login --username {shreepardeshi}')\n",
        "\n",
        "# Tag the Docker image\n",
        "os.system(f'docker tag research-titles {shreepardeshi}/research-titles:latest')\n",
        "\n",
        "# Push the Docker image\n",
        "os.system(f'docker push {shreepardeshi}/research-titles:latest')"
      ],
      "metadata": {
        "id": "onRfhuI7aptB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 7:\n",
        "Now you need to build your own chatbot using the seq2seq model of\n",
        "Amazon website by scrape the website and containerize the application and push\n",
        "to public docker hub"
      ],
      "metadata": {
        "id": "-miVpLyLa1wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rasa"
      ],
      "metadata": {
        "id": "bclhMOCsdRyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "from rasa import RasaClient\n",
        "import subprocess\n",
        "\n",
        "# Scrape the Amazon website to get the product descriptions\n",
        "def get_product_descriptions(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    product_descriptions = []\n",
        "    for product in soup.find_all(\"div\", class_=\"a-section a-spacing-medium\"):\n",
        "        product_descriptions.append(product.find(\"div\", class_=\"a-row a-size-base\").text)\n",
        "    return product_descriptions\n",
        "\n",
        "# Train a seq2seq model on the product descriptions\n",
        "def train_seq2seq_model(product_descriptions):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(len(product_descriptions), 128),\n",
        "        tf.keras.layers.LSTM(128),\n",
        "        tf.keras.layers.Dense(len(product_descriptions))\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "    model.fit(product_descriptions, product_descriptions, epochs=10)\n",
        "    return model\n",
        "\n",
        "# Create a chatbot that can generate responses to user queries\n",
        "def create_chatbot(seq2seq_model):\n",
        "    intents = {\n",
        "        \"greeting\": [\"Hi\", \"Hello\", \"How are you?\"],\n",
        "        \"goodbye\": [\"Bye\", \"See you later\", \"Talk to you soon\"],\n",
        "        \"product_info\": [\"What is the price of <product_name>?\", \"What are the features of <product_name>?\", \"Where can I buy <product_name>?\"]\n",
        "    }\n",
        "    entities = {\n",
        "        \"product_name\": [\"<product_name>\"]\n",
        "    }\n",
        "    dialog_flow = RasaClient(\"./models\")\n",
        "    dialog_flow.train(intents, entities)\n",
        "    return dialog_flow\n",
        "\n",
        "# Containerize the chatbot application\n",
        "def containerize_chatbot(chatbot):\n",
        "    with open(\"Dockerfile\", \"w\") as f:\n",
        "        f.write(\"\"\"\n",
        "FROM python:3.8\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY requirements.txt ./\n",
        "\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "COPY . .\n",
        "\n",
        "CMD [\"python\", \"app.py\"]\n",
        "\"\"\")\n",
        "    subprocess.run([\"docker\", \"build\", \"-t\", \"chatbot\", \".\"])\n",
        "    return chatbot\n",
        "\n",
        "# Push the chatbot application to Docker Hub\n",
        "def push_chatbot_to_docker_hub():\n",
        "    subprocess.run([\"docker\", \"push\", \"chatbot\"])\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Scrape the Amazon website to get the product descriptions\n",
        "    url = \"https://www.amazon.com/s?k=laptop\"\n",
        "    product_descriptions = get_product_descriptions(url)\n",
        "\n",
        "    # Train a seq2seq model on the product descriptions\n",
        "    seq2seq_model = train_seq2seq_model(product_descriptions)\n",
        "\n",
        "    # Create a chatbot that can generate responses to user queries\n",
        "    chatbot = create_chatbot(seq2seq_model)\n",
        "\n",
        "    # Containerize the chatbot application\n",
        "    containerize_chatbot(chatbot)\n",
        "\n",
        "    # Push the chatbot application to Docker Hub\n",
        "    push_chatbot_to_docker_hub()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "99cHhk0jchZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8:\n",
        " Take a any own dataset and build a knowledge bot"
      ],
      "metadata": {
        "id": "wNsVGYlNgLqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"dialogs.txt\", delimiter=\"\\t\", header=None)\n",
        "dialogs = df[0].tolist()\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a function to generate responses\n",
        "def generate_response(question):\n",
        "    input_text = \"User: \" + question + \"\\nBot:\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=50,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    response = tokenizer.decode(generated_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test the knowledge bot\n",
        "while True:\n",
        "    question = input(\"You: \")\n",
        "    if question.lower() == \"bye\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    response = generate_response(question)\n",
        "    print(\"Chatbot:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEjDuzZ9gOlk",
        "outputId": "a16c76d1-e9aa-4c03-b429-3b0bcb590e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "Chatbot:  Hello, hello!\n",
            "Bot: Hello, hello!\n",
            "Bot: Hello, hello!\n",
            "Bot: Hello, hello!\n",
            "Bot: Hello, hello!\n",
            "Bot: Hello, hello!\n",
            "Bot: Hello,\n",
            "You: hi , how are you doing ?\n",
            "Chatbot:  I'm doing really well. I'm not sure what I'm going to do with my life now.\n",
            "Bot: It's not going to be long, but I'm going to be\n",
            "You: what school do you go to ?\t\n",
            "Chatbot:  I'm going to go to a school in the Philippines. It's called the Philippines.\n",
            "Bot: And I'm going to have to get a job in that country.\n",
            "Bot\n",
            "You: do you like it there ?\t\n",
            "Chatbot:  i don't.\n",
            "Bot: i like it there.\n",
            "Bot: i do.\n",
            "Bot: i like it there.\n",
            "Bot: i like it there.\n",
            "Bot: i\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 9:"
      ],
      "metadata": {
        "id": "rRsxTnnYpUsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "8zjGbzTbf_lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the Wav2Vec model and tokenizer\n",
        "wav2vec_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "wav2vec_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "\n",
        "# Load the T5 model and tokenizer\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Transcribe audio file to text\n",
        "audio_file = \"/content/smooth-ac-guitar-loop-93bpm-137706.mp3\"\n",
        "speech, _ = torchaudio.load(audio_file)\n",
        "input_values = wav2vec_tokenizer(speech, return_tensors=\"pt\").input_values\n",
        "logits = wav2vec_model(input_values).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = wav2vec_tokenizer.batch_decode(predicted_ids)[0]\n",
        "\n",
        "# Translate text to a different language\n",
        "target_language = \"fr\"  # Specify the target language code\n",
        "translation_text = f\"translate English to {target_language}: {transcription}\"\n",
        "input_ids = t5_tokenizer.encode(translation_text, return_tensors=\"pt\")\n",
        "translated_ids = t5_model.generate(input_ids)\n",
        "translation = t5_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Convert translated text to audio file\n",
        "translated_speech = t5_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "translated_audio = t5_model.tts.translate(translated_speech, lang=target_language)\n",
        "translated_audio.export(\"translated_audio.wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "MV0Vd4eIhAmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 10:"
      ],
      "metadata": {
        "id": "jHP0ShN6w0gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install flask"
      ],
      "metadata": {
        "id": "BgGV0qn50SN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Define API endpoint for auto-correction\n",
        "@app.route('/autocorrect', methods=['POST'])\n",
        "def autocorrect():\n",
        "    text = request.json['text']\n",
        "    \n",
        "    # Perform auto-correction logic using NLP\n",
        "    \n",
        "    corrected_text = perform_autocorrection(text)\n",
        "    \n",
        "    # Return the corrected text as JSON response\n",
        "    return jsonify({'corrected_text': corrected_text})\n",
        "\n",
        "def perform_autocorrection(text):\n",
        "    # Implement your NLP auto-correction logic here\n",
        "    # You can use libraries like spaCy, NLTK, or custom algorithms\n",
        "    \n",
        "    # Placeholder implementation: return the input text as is\n",
        "    return text\n",
        "\n",
        "# Run the Flask app\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YR1zUaC0N6o",
        "outputId": "46e54d7a-2627-4df2-97c7-244c090f2004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to deploy the application on Heroku:\n"
      ],
      "metadata": {
        "id": "LXEpf8l6z0kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "MoN-5ZO5zdN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a .env file and add the following environment variables:"
      ],
      "metadata": {
        "id": "lSFZYt0Iz7Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLASK_APP=app.py\n",
        "FLASK_ENV=development"
      ],
      "metadata": {
        "id": "nzC4Iw5lz5eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the following commands in the terminal:"
      ],
      "metadata": {
        "id": "ZJa_2Ybkz9fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heroku create\n",
        "heroku config:set FLASK_APP=app.py FLASK_ENV=development\n",
        "git add .\n",
        "git commit -m \"initial commit\"\n",
        "git push heroku master\n",
        "heroku open"
      ],
      "metadata": {
        "id": "qnalY7Lxz-57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://your-heroku-app-url/correct'\n",
        "data = {'text': 'Thiss is a testt sentennce'}\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "print(response.json()['corrected_text'])"
      ],
      "metadata": {
        "id": "0JhHys4V0Bo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}